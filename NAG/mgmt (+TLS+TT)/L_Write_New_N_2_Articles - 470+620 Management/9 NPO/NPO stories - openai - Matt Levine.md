
## AI sorting

A dumb simple model of artificial intelligence companies is:

1. It would be good to develop good AI (AI that helps humans), but bad to develop bad AI (AI that kills or enslaves humans).
2. If you try to build good AI, there is some risk of building bad AI instead (your robot tricks you into thinking that it’s nice, then enslaves you), so you have to be very very careful. You can’t move too fast; you have to check carefully, at each step, to make sure that your robot is not secretly evil.
3. Company A is formed by idealistic AI researchers who want to create good AI. They work together well for a while.
4. Disagreements develop. Some researchers at Company A say “we need to work faster to build good AI, because if we don’t, someone else will come along and build bad AI first instead.” Others say “no, we can’t work faster, that would compromise our ability to check that the robot is not evil.” 
5. The first group wins the argument, for reasons. [[5]](imap://dave%40stucky%2Etech@mail.stucky.tech:993/fetch%3EUID%3E.INBOX%3E6526#footnote-5)
6. The people who lose the argument, who are genuinely worried about bad AI, quit Company A in outrage and go start Company B, with the goal of carefully and safely creating good AI.
7. They work together well for a few months.
8. Disagreements develop at Company B. Some researchers say “we need to work faster to build good AI, because otherwise _Company A_ will build bad AI first. That’s why we quit, after all.” Others say “no, we can’t work faster, that would compromise our bad robot checks. _That’s_ why we quit, after all.”
9. The first group wins the argument, for the same reasons as in Step 5.
10. The people who lose the argument quit and start Company C.
11. This keeps repeating: Company C eventually splits over similar tensions, but also Company A and Company B can themselves keep dividing as some people want to move faster than others.
12. Eventually all the AI researchers are very finely sorted by aggressiveness, so that Company Z is full of purists who are too cautious ever to build anything at all, while Company A is full of people who are like “actually being enslaved by robots would be pretty cool.”

This is not accurate in all respects — sometimes the second group [wins the argument for a weekend!](https://link.mail.bloombergbusiness.com/click/35781629.270764/aHR0cHM6Ly93d3cuYmxvb21iZXJnLmNvbS9vcGluaW9uL2FydGljbGVzLzIwMjMtMTEtMjAvd2hvLWNvbnRyb2xzLW9wZW5haT9jbXBpZD1CQkQwNjIwMjRfTU9ORVlTVFVGRiZ1dG1fbWVkaXVtPWVtYWlsJnV0bV9zb3VyY2U9bmV3c2xldHRlciZ1dG1fdGVybT0yNDA2MjAmdXRtX2NhbXBhaWduPW1vbmV5c3R1ZmY/60e87ce39a995a4b1a2deb96Bc6e66e59) — but it is an intuitive model that helps to explain [stuff like this](https://link.mail.bloombergbusiness.com/click/35781629.270764/aHR0cHM6Ly93d3cuYmxvb21iZXJnLmNvbS9uZXdzL2FydGljbGVzLzIwMjQtMDYtMTkvb3BlbmFpLWNvLWZvdW5kZXItcGxhbnMtbmV3LWFpLWZvY3VzZWQtcmVzZWFyY2gtbGFiP2NtcGlkPUJCRDA2MjAyNF9NT05FWVNUVUZGJnV0bV9tZWRpdW09ZW1haWwmdXRtX3NvdXJjZT1uZXdzbGV0dGVyJnV0bV90ZXJtPTI0MDYyMCZ1dG1fY2FtcGFpZ249bW9uZXlzdHVmZg/60e87ce39a995a4b1a2deb96B04177745):

> For the past several months, the question “Where’s Ilya?” has become a common refrain within the world of artificial intelligence. Ilya Sutskever, the famed researcher who co-founded OpenAI, took part in the 2023 board ouster of Sam Altman as chief executive officer, before changing course and helping engineer Altman’s return. From that point on, Sutskever went quiet and left his future at OpenAI shrouded in uncertainty. Then, in mid-May, Sutskever announced his departure, saying only that he’d disclose his next project “in due time.”
> 
> Now Sutskever is introducing that project, a venture called Safe Superintelligence Inc. aiming to create a safe, powerful artificial intelligence system within a pure research organization that has no near-term intention of selling AI products or services. In other words, he’s attempting to continue his work without many of the distractions that rivals such as OpenAI, Google and Anthropic face. “This company is special in that its first product will be the safe superintelligence, and it will not do anything else up until then,” Sutskever says in an exclusive interview about his plans. “It will be fully insulated from the outside pressures of having to deal with a large and complicated product and having to be stuck in a competitive rat race.”

OpenAI [was founded](https://link.mail.bloombergbusiness.com/click/35781629.270764/aHR0cHM6Ly93d3cuYmxvb21iZXJnLmNvbS9vcGluaW9uL2FydGljbGVzLzIwMjQtMDMtMDEvb3BlbmFpLWlzbi10LW9wZW4tZW5vdWdoLWZvci1lbG9uP2NtcGlkPUJCRDA2MjAyNF9NT05FWVNUVUZGJnV0bV9tZWRpdW09ZW1haWwmdXRtX3NvdXJjZT1uZXdzbGV0dGVyJnV0bV90ZXJtPTI0MDYyMCZ1dG1fY2FtcGFpZ249bW9uZXlzdHVmZg/60e87ce39a995a4b1a2deb96B13a95f24) to build artificial general intelligence safely, free of outside commercial pressures. And now every once in a while it [shoots out](https://link.mail.bloombergbusiness.com/click/35781629.270764/aHR0cHM6Ly90aW1lLmNvbS82OTgzNDIwL2FudGhyb3BpYy1zdHJ1Y3R1cmUtb3BlbmFpLWluY2VudGl2ZXMv/60e87ce39a995a4b1a2deb96B5889169d) a new AI firm whose mission is to build artificial general intelligence safely, free of the commercial pressures at OpenAI.
