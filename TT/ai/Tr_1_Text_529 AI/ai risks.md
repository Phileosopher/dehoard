
What if a phishing email appears to come from your credit card company and is followed up by a phone call from a friendly human voice that you can't tell is AI-generated?

Once AI becomes able to generate fully realistic fake videos of you committing crimes, will you vote for a system where the government tracks everyone's whereabouts at all times and can provide you with an ironclad alibi if needed?

If a self-driving car causes an accident, who should be liable? The car itself!
Self-driving cars might be allowed (and required) to hold car insurance.
Models with a sterling safety record will qualify for premiums that are very low, probably lower than what's available to human drivers, while poorly designed models from sloppy manufacturers will only qualify for insurance policies that make them prohibitively expensive to own.

If machines such as cars are allowed to hold insurance policies, should they also be able to own money and property?
If so, there's nothing legally stopping smart computers from making money.
If AI systems eventually get better than humans at investing (which they already are in some domains), this could lead to a situation where most of our economy is owned and controlled by machines.
Most of our economy is already owned by another form of non-human entity: corporations.

In a dogfight between a fully autonomous drone that can respond instantly and a drone reacting more sluggishly because it's remote-controlled by a human halfway around the world, which one do you think would win?

Those who stand to gain most from an arms race aren't superpowers but small rogue states and non-state actors such as terrorists, who gain access to the weapons via the black market once they've been developed.

Once mass-produced, small AI-powered killer drones are likely to cost little more than a smartphone.
Whether it's a terrorist wanting to assassinate a politician or a jilted lover seeking revenge on his ex-girlfriend, all they need to do is upload their target's photo and address into the killer drone: it can then fly to the destination, identify and eliminate the person, and self-destruct to ensure that nobody knows who was responsible.

Would it be easier to enforce a requirement that enemy autonomous weapons be 100% ethical than to enforce that they aren't produced in the first place?

I don't worry about turning AI evil now for the same reason I don't worry about the problem of overpopulation on the planet Mars.
