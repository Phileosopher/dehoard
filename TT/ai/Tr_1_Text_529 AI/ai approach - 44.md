
Most AI researchers and neuroscientists guess that the quickest route to superintelligence is to bypass brain emulation and engineer it in some other way.
After all, why should our simplest path to a new technology be the one that evolution came up with.
The aviation industry didn't start with mechanical birds.
- THIS IS FUNNY BECAUSE THE WRIGHT BROTHERS SUCCEEDED BY OBSERVING BIRDS ALL DAY

A fast AI takeoff makes world takeover easier, while a slow one makes an outcome with many competing players more likely.

Consciousness is by far the most remarkable trait.
It's how our Universe gets meaning. Galaxies are beautiful only because we see and subjectively experience them.
If in the distant future our cosmos has been settled by high-tech zombie AIs, then it doesn't matter how fancy their intergalactic architecture is: it won't be beautiful or meaningful, because there's nobody and nothing to experience it - it's all just a huge and meaningless waste of space.

There are computer tournaments in so-called losing chess.

A human-extinction scenario that some people may feel better about: viewing the AI as our descendants.
Parents with a child smarter than them, who learns from them and accomplishes what they could only dream of, are likely happy and proud even if they know they can't live to see it all.
In this spirit, AIs replace humans but give us a graceful exit that makes us view them as our worthy descendants.
Humans are gradually phased out via a global one-child policy, but are treated so exquisitely well until the end that they feel they're in the most fortunate generation ever.
As long as the AIs eliminate poverty and give all humans the opportunity to live full and inspiring lives.

The only viable path to broad relinquishment of technology is to enforce it through a global totalitarian state.
If some but not all relinquish a transformative technology, then the nations or groups that defect will gradually gain enough wealth and power to take over.

Unambitious civilizations simply become cosmically irrelevant.
Almost all life that exists will be ambitious life.

There are two mathematically equivalent ways of describing each physical law: either as the past causing the future, or as nature optimizing something.
The second way is more elegant and profound.

Goal-oriented behavior was hardwired in the very laws of physics.
To rescue a swimmer as fast as possible, a lifeguard won't go in a straight line, but a bit further along the beach where she can go faster than in the water.
(Nature does this too.)

The second law of thermodynamics states that entropy tends to increase until it reaches its maximum possible value.
When you pour cold milk into hot coffee, for example, your beverage appears to march irreversibly toward its own personal heat death goal, and before long, it's all just a uniform lukewarm mixture.
If a living organism dies, its entropy also starts to rise, and before long, the arrangement of its particles tends to get much less organized.

Gravity behaves differently from all other forces and strives to make our Universe not more uniform and boring but more clumpy and interesting.

Dissipation-driven adaptation:
Random groups of particles strive to organize themselves so as to extract energy from their environment as efficiently as possible.
Molecules exposed to sunlight would over time tend to arrange themselves to get better and better at absorbing sunlight.
In other words, nature appears to have a built-in goal of producing self-organizing systems that are increasingly complex and lifelike.

The second law of thermodynamics has a life loophole: although the total entropy must increase, it's allowed to decrease in some places as long as it increases even more elsewhere.
So life maintains or increases its complexity by making its environment messier.

There are many known examples of such emergent self-replication. For example:
Vortices in turbulent fluids can make copies of themselves, and clusters of microspheres can coax nearby spheres into forming identical clusters.
At some point, a particular arrangement of particles got so good at copying itself that it could do so almost indefinitely by extracting energy and raw materials from its environment.
We call such a particle arrangement life.

A living organism is an agent of bounded rationality that doesn't pursue a single goal, but instead follows rules of thumb for what to pursue and avoid.
Our human minds perceive these evolved rules of thumb as feelings, which guide our decision making toward the ultimate goal of replication.
Feelings of hunger and thirst protect us from starvation and dehydration, feelings of pain protect us from damaging our bodies, feelings of lust make us procreate, feelings of love and compassion make us help other carriers of our genes and those who help them and so on.
Guided by these feelings, our brains can quickly and efficiently decide what to do without having to subject every choice to a tedious analysis of its ultimate implications for how many descendants we'll produce.
The ultimate authority is now our feelings, not our genes.
Human behavior strictly speaking doesn't have a single well-defined goal at all.

If you'd been observing Earth's atoms since our planet formed, you'd have noticed three stages of goal-oriented behavior:
1. All matter seemed focused on dissipation (entropy increase).
2. Some of the matter came alive and instead focused on replication and subgoals of that.
3. A rapidly growing fraction of matter was rearranged by living organisms to help accomplish their goals.

The real risk with Artificial General Intelligence isn't malice but competence.
A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren't aligned with ours, we're in trouble.

Three tough subproblems:
1. Making AI learn our goals
2. Making AI adopt our goals
3. Making AI retain our goals

Midas asked that everything he touched turn to gold, but was disappointed when this prevented him from eating and even more so when he inadvertently turned his daughter to gold.
In the stories where a genie grants three wishes, there are many variants for the first two wishes, but the third wish is almost always the same: "Please undo the first two wishes, because that's not what I really wanted."
To figure out what people really want, you can't merely go by what they say.
You also need a detailed model of the world, including the many shared preferences that we tend to leave unstated because we consider them obvious.
Once we have such a world model, we can often figure out what people want even if they don't tell us, simply by observing their goal-oriented behavior.

Children of hypocrites learn more from what they see their parents do than from what they hear them say.

We are currently trying hard to enable machines to infer goals from behavior, and this will be useful also long before any superintelligence comes on the scene.
For example, a retired man may appreciate it if his eldercare robot can figure out what he values simply by observing him, so that he's spared the hassle of having to explain everything with words or computer programming.
One challenge involves finding a good way to encode arbitrary systems of goals and ethical principles into a computer.
Another challenge is making machines that can figure out which particular system best matches the behavior they observe.
Inverse reinforcement learning is that we make decisions all the time, and that every decision we make reveals something about our goals.
By observing lots of people in lots of situations (either for real or in movies and books), the AI can eventually build an accurate model of all our preferences.

The time window during which you can load your goals into an AI may be quite short: the brief period between when it's too dumb to get you and too smart to let you.

A superintelligent AI will resist being shut down if you give it any goal that it needs to remain operational to accomplish - and this covers almost all goals!
If you give a superintelligence the sole goal of minimizing harm to humanity, for example, it will defend itself against shutdown attempts because it knows we'll harm one another much more in its absence through future wars and other follies.

The propensity to change goals in response to new experiences and insights increases rather than decreases with intelligence.

The ethical views of many thinkers can be distilled into four principles:
• Utilitarianism: Positive conscious experiences should be maximized and suffering should be minimized.
• Diversity: A diverse set of positive experiences is better than many repetitions of the same experience, even if the latter has been identified as the most positive experience possible.
• Autonomy: Conscious entities/societies should have the freedom to pursue their own goals unless this conflicts with an overriding principle.
• Legacy: Compatibility with scenarios that most humans today would view as happy, incompatibility with scenarios that essentially all humans today would view as terrible.

Would we really want people from 1,500 years ago to have a lot of influence over how today's world is run?
If not, why should we try to impose our ethics on future beings that may be dramatically smarter than us?

If some sophisticated future computer programs turn out to be conscious, should it be illegal to terminate them?
If there are rules against terminating digital life forms, then need there also be restrictions on creating them to avoid a digital population explosion?

A fast-forward replay of our 13.8-billion-year cosmic history:
1. Matter seemingly intent on maximizing its dissipation
2. Primitive life seemingly trying to maximize its replication
3. Humans pursuing not replication but goals related to pleasure, curiosity, compassion and other feelings that they'd evolved to help them replicate
4. Machines built to help humans pursue their human goals

The only currently programmable goals that are guaranteed to remain truly well-defined as an AI gets progressively more intelligent are goals expressed in terms of physical quantities alone, such as particle arrangements, energy and entropy.
However, humans are a historical accident, and aren't the optimal solution to any well-defined physics problem.

How should we strive to shape the future of our Universe?
If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us.
This makes it timely to rekindle the classic debates of philosophy and ethics, and adds a new urgency to the conversation!
Philosophy with a deadline.

Many arguments generate more heat than light.
(because the antagonists are talking past each other.)

Galileo described nature as "a book written in the language of mathematics."

Scientists started taking Newton's theory of gravity seriously because they got more out of it than they put into it.
Simple equations could accurately predict the outcome of every gravity experiment ever conducted.

Emergent phenomenon has properties above and beyond those of its particles.
Wetness: A drop of water is wet, but an ice crystal and a cloud of steam aren't, even though they're made of identical water molecules.
Why? Because the property of wetness depends only on the arrangement of the molecules, the phenomenon of wetness emerges only when there are many molecules, arranged in the pattern we call liquid.
What particle arrangements are conscious?
Consciousness is an emergent phenomenon.
Consciousness is the way that information feels when it's processed in certain ways.
It must be substrate-independent; it's only the structure of the information processing that matters, not the structure of the matter doing the information processing.

Which particle arrangements are conscious and which aren't?
If we can answer that, then we can figure out which AI systems are conscious.
It can also help emergency-room doctors determine which unresponsive patients are conscious.

We may sometimes have "consciousness without access," that is, subjective experience of things that are too complex to fit into our working memory for later use.
For example, when you experience inattentional blindness by being too distracted to notice an object in plain sight, this doesn't imply that you had no conscious visual experience of it, merely that it wasn't stored in your working memory.
Should it count as forgetfulness rather than blindness?

When people ask about the meaning of life as if it were the job of our cosmos to give meaning to our existence, they're getting it backward.
It's not our Universe giving meaning to conscious beings, but conscious beings giving meaning to our Universe.

Contrast sapience (the ability to think intelligently) with sentience (the ability to subjectively experience qualia).
Humans have built our identity on being Homo sapiens.
I suggest that we rebrand ourselves as Homo sentiens.

Science gathers knowledge faster than society gathers wisdom.

Mindful optimism is the expectation that good things will happen if you plan carefully and work hard for them.

Develop positive visions for the future.
Positive visions form the foundation of all collaboration.
After all, why sacrifice something you have if you can't imagine the even greater gain that this will provide?
This means that we should be imagining positive futures not only for ourselves, but also for society and for humanity.

Do you want to own your technology or do you want your technology to own you?

The apparent consciousness of the octopus, with which we share no recent common ancestor, suggests consciousness is more than coincidence.
- THIS PRESUMES WE WEREN'T CREATED LOL

For ages, we tried to fly by copying what birds do, but when we finally did learn to fly, it was not by copying birds.
There are still things we don't understand about how birds fly, yet we can now fly further and faster than they can.
