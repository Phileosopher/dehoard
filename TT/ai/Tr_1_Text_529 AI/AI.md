
AI is essentially reproducing "average" behavior
- it isn't going to create anything that hasn't been done many times before
- it's a dumb computer still, so it's just following rules and does NOT have an intuition for anything
- any assertions to the contrary are [trends] spurting up by self-interested individuals who want to sell something

AI hasn't been new
- 1966 saw machine translation
- it was a total dud, with lots of hype that went nowhere
- these trends create an "AI winter" where the trend and hype dies, so funding gets cut
- the entire thing is driven by a misrepresentation of what constitutes a human soul (i.e., naturalism), and how it can be reproduced
- we can reproduce intelligent behavior, but it's always dumb as it does it
- Lisp was originally the program of the artificial intelligence language
    - it failed, and we're left with emacs
    - in the late 1980s, the AI revolution in Japan collapsed, and all venture capital that even MENTIONED ai was rejected outright as "wide-eyed dreaming"

Black box vs explainable AI
- black box AI is popular, and the designers CAN'T describe what happens because the [algorithm] has too many complexities to reliably tell them (so they have to approximately guess where-ish a bug might be).
- explainable AI (XAI) is easy to [diagnose], by contrast, but also will quickly reveal any [agenda](bad systems) an individual or organization may have.

If you get past the hype, AI does 1 thing now that changes everything:
- It allows all the mundane tasks the computer used to do be much more powerful toward small permutations.
- Very specific, very analytical tasks now have the power to be parsed or compiled more easily.

AI is a bit like [engineering] in general
- we don't interact with the result directly, but are able to have a secondary interaction with it using a tool
    - that tool isn't "us", but we still command it and there is a defined order that clarifies how it's built
    - therefore, we never suspect an engineered object will go out of control without a clear and obvious reason
- in AI, we have a secondary interaction with a logical extension, which is still bound by some form of logic
    - that tool isn't "our thinking", but we still command it and there is a defined order that clarifies its logic
    - therefore, we should never suspect an AI will go out of control without a clear and obvious originating external cause

[A guide to why advanced AI could destroy the world - Vox](https://www.vox.com/the-highlight/23447596/artificial-intelligence-agi-openai-gpt3-existential-risk-human-extinction)
- reminiscent of CGP Grey's "Humans Need Not Apply"
- the problem is that it's logic-based emotions (very granular) instead of emotions-based logic (i.e., things even toddlers can outperform adequately-enough)
- the modularity of human behavior is its engineered advantage over a specialized computer implementation

[Instrumental convergence - Wikipedia](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer)
- i.e., even AI are subject to aspects of perverse incentive

---

The "how" of machine learning, as an abstraction, is pretty easy to understand:
1. Give a computer many, many instances of a labeled thing (e.g., a cat photo).
2. Eventually, the computer makes rules that it cross-references that define those instances (e.g., fuzzy-looking, ear shape).
3. After enough crap thrown at it, it'll be able to get reliable at guessing (e.g., 80.2% accurate).
4. It's relatively trivial to make it rebuild a variation at this point.

This basically means "averaged-out" behavior, so it'll do average everything.

The implications, then, are reasonable to understand:
1. It can find all sorts of materials that were never discovered, just because it's filling in the blanks.
2. It can make average art, which cherry-picked good stuff in it (e.g., @CSB)
3. It can make documents or coding, but probably needs a human to review it.
4. It'll make a hell of a security camera.
5. It's still a dumb computer.

---

AI is largely a branding, a bit like "cloud" or ".com" was
- from a [marketing] standpoint, anything logic-based can be spun as "AI"
- the only "true" AI, though, is machine learning, and it's still not really technically "intelligence" in the way we interpret the definition of [intelligence as determined by sentient people](humanity)
