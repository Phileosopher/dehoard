
[OPT: Open Pre-trained Transformer Language Models | Hacker News](https://news.ycombinator.com/item?id=31243569)
[[2205.01068] OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)

[Does offering ChatGPT a tip cause it to generate better text? | Hacker News](https://news.ycombinator.com/item?id=39495476)
[Does Offering ChatGPT a Tip Cause it to Generate Better Text? An Analysis | Max Woolf's Blog](https://minimaxir.com/2024/02/chatgpt-tips-analysis/)

[The Waluigi Effect | Hacker News](https://news.ycombinator.com/item?id=35042431)
[The Waluigi Effect (mega-post) - LessWrong](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post)

[Show HN: BBC "In Our Time", categorised by Dewey Decimal, heavy lifting by GPT | Hacker News](https://news.ycombinator.com/item?id=35073603)
[Explore the In Our Time archive | Braggoscope](https://www.braggoscope.com/)

[Ask HN: How are you using GPT to be productive? | Hacker News](https://news.ycombinator.com/item?id=35299071)

[I wish GPT4 had never happened | Hacker News](https://news.ycombinator.com/item?id=35492730)
[I wish GPT4 had never happened](https://chaudhry.notion.site/I-wish-GPT4-had-never-happened-9f0cbf2848a44ec9911c07fb34ff5de3)

[A Baby GPT | Hacker News](https://news.ycombinator.com/item?id=35506069)
[Andrej Karpathy on X: "This is a baby GPT with two tokens 0/1 and context length of 3, viewing it as a finite state markov chain. It was trained on the sequence "111101111011110" for 50 iterations. The parameters and the architecture of the Transformer modifies the probabilities on the arrows. E.g. weâ€¦ https://t.co/vj10nZEXlH" / X](https://twitter.com/karpathy/status/1645115622517542913)

[GPT detectors are biased against non-native English writers | Hacker News](https://news.ycombinator.com/item?id=36019580)
[[2304.02819] GPT detectors are biased against non-native English writers](https://arxiv.org/abs/2304.02819)

[Exploring GPTs: ChatGPT in a trench coat? | Hacker News](https://news.ycombinator.com/item?id=38277926)
[Exploring GPTs: ChatGPT in a trench coat?](https://simonwillison.net/2023/Nov/15/gpts/)

[Lessons after a Half-billion GPT Tokens | Hacker News](https://news.ycombinator.com/item?id=40015185)
[Lessons after a half-billion GPT tokens - Ken Kantzer's Blog](https://kenkantzer.com/lessons-after-a-half-billion-gpt-tokens/)
