
[Consistency LLM: converting LLMs to parallel decoders accelerates inference 3.5x | Hacker News](https://news.ycombinator.com/item?id=40302201)
[Consistency Large Language Models: A Family of Efficient Parallel Decoders | Hao AI Lab @ UCSD](https://hao-ai-lab.github.io/blogs/cllm/)
